{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическое задание\n",
    "## Метод обратного распространения ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### О задании\n",
    "\n",
    "В этом задание вы:\n",
    "* познакомитесь с методом обратного распространения ошибки \n",
    "* реализуете прямой проход и обратный проход в нейросети\n",
    "* реальзуете стохастический градиентный спуск с моментов\n",
    "* обучите нейросеть для классификации рукописных цифр"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 1. Прямой и обратный проход нейросети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Теоретическая часть\n",
    "\n",
    "<p>\n",
    "Метод обратного распространения ошибки — это метод обучения многослойной нейрононной сети, который впервые был открыт двумя независимыми группами исследователей в 1974 г. Этот метод определяет алгоритм эффективного вычисления градиентов параметров нейронной сети, что позволяется применить метод градиентного спуска в задачи минимизации функционала ошибки. \n",
    "</p>\n",
    "\n",
    "Давайте рассморим M-слойную полносвязную нейронную сеть.\n",
    "\n",
    "<img src=\"network.png\" width=\"800\">\n",
    "\n",
    "На рисунке верхний индекс всегда используется для обозначения номера слоя нейросети. Рассмотрим некоторый n-ый слой, который назовем текущим. Данный слой на вход принимает\n",
    "$N^{(n-1)}$, \n",
    "признаков \n",
    "$y^{(n - 1)}_i$, $i=\\overline{1\\mathinner {\\ldotp \\ldotp}N^{(n-1)}}$, \n",
    "которые являются значения выходов нейронов предыдущего слоя, и \n",
    "$y^{(n-1)}_0=1$\n",
    "(смещение или bias нейрона — константа, которая рассматривается как вход нейрона для упрощения записи дальнейших вычислений). Вес нейрона, связывающий $i$'ый нейрон предыдущего слоя с $j$'ым нейроном текущего, обозначен $w^{(n)}_{ij}$. \n",
    "За \n",
    "$z_j^{(n)}=\\sum_{i=0}^{N^{(n-1)}}{w_{ij}^{(n)}y_i^{(n-1)}}$, \n",
    "обозначена линейная комбинация входов и весов.\n",
    "$\\sigma^{(n)}_j$ \n",
    "— функция активации j'ого нейрона(так как функции активации нейронов в общем случае могут быть различны), а \n",
    "$y_j^{(n)} = \\sigma_j^{(n)}(z_j^{(n)}) = \\sigma_j^{(n)}(\\sum_{i=0}^{N^{(n-1)}}{w_{ij}^{(n)}y_i^{(n-1)}}) $ \n",
    "— значение функции активации или выход j'ого нейрона. \n",
    "\n",
    "<img src=\"layer.png\" width=\"800\">\n",
    "\n",
    "Резюмируем обозначения:\n",
    "* $M$ - колличество слоев\n",
    "* $N^{(n)}$ - колличество нейронов в $n$-ом слое\n",
    "* $\\{ w_{ij}^{(n)}\\}$ - веса нейронов $n$-ого слоя\n",
    "* $z_j^{(n)} = \\sum_{i=0}^{N^{(n - 1)}}{w_{ij}^{(n)}y_i^{(n-1)}}$\n",
    "* $\\sigma_j^{(n)}$ - функция активации $j$-ого нейрона $n$-ого слоя\n",
    "* $y_j^{(n)} = \\sigma_j^{(n)}(z_j^{(n)})$ - выход $j$-ого нейрона $n$-ого слоя \n",
    "\n",
    "За $E(\\boldsymbol{\\widehat{y}}, \\boldsymbol{y})$  обозначим некоторую диффенцируюмую функцию ошибки. Здесь  <br/>\n",
    "$\\boldsymbol{\\widehat{y}} = (y_1, ..., y_{N^{(M)}})$ - значение целевой переменной,  \n",
    "$\\boldsymbol{y} = (y_1^{(M)}, ..., y_{N^{(M)}}^{(M)}))$ - выход нейросети. \n",
    "\n",
    "Давайте попробуем оценить сложность вычисление частной производной функции ошибки $E$. Предположим, что сложность вычисления частной производной функции в точке приблизительно равна сложности вычисления самой функции в точке. Пусть нейросеть имеет M полносвязных слоем по N нейронов в каждом слое. Тогда:\n",
    "* $O(N)$ - cложность вычисления одного выхода одного слоя (перемножение N весов на N входов)\n",
    "* $O(N^2)$ - cложность вычисления всех выходов одного слоя\n",
    "* $O(M * N^2)$ - cложность вычисления функции ошибки (последовательно вычисляем выходы M слоев)\n",
    "* $O(M^2 * N^4)$ - cложность частных производных функции ошибки по всем весам (всего  $O(M * N^2)$ весов)\n",
    "\n",
    "Получается для нейросети, состоящей из одного слоя с 1000 нейронами, сложность вычисления градиента должна быть равна $O(10^{12})$. А это уже невероятно много.\n",
    "\n",
    "Идея метода эффективного расчета градиентов заключается в том, чтобы при прямом проходе нейросети сохранить некоторые вычисленные значения, которые потом позволят быстро находить градиент."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычисление градиента"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Будем вычислять частные производные функции ошибки от последних слоев в первым.\n",
    "Помимо частных производных \n",
    "$$\\frac{\\partial}{\\partial w_{ij}^{(n)}} E$$ \n",
    "будем вычислять значения производных \n",
    "$$\\frac{\\mathrm{\\partial}}{\\partial y_i^{(n)}} E \\quad \\mathrm{и} \\quad \\frac{\\mathrm{\\partial}}{\\partial z_i^{(n)}} E$$ вычисление которых является одним из ключевых моментов в алгоритме обратном распространения ошибки. \n",
    "\n",
    "Полезно посмотреть как аналитически выглядит вычисления выхода нейросети \n",
    "$\\boldsymbol{\\widehat{y}}=(y_1^{(M)}, ..., y_{N^{(M)}}^{(M)})$.:\n",
    "\n",
    "$$ \\widehat{y_j}=y_j^{(M)}=\\sigma_j^{(M)}(\\sum_{i=0}^{N^{(M-1)}}{w_{ij}^{(M)}\\sigma_i^{(M-1)}(\\sum_{k=0}^{N^{(M-2)}}{w_{ki}^{(M-1)}\\sigma_k^{(M-2)}(...)})})$$\n",
    "\n",
    "В случае однослойной нейросети, получим: \n",
    "\n",
    "$$ \\widehat{y_j}=\\sigma_j^{(1)}(\\sum_{i=0}^{N^{(0)}}{w_{ij}^{(1)}x_j}),$$\n",
    "\n",
    "двухслойной - \n",
    "\n",
    "$$ \\widehat{y_j}=\\sigma_j^{(2)}(\\sum_{i=0}^{N^{(1)}}{w_{ij}^{(2)}\\sigma_i^{(1)}(\\sum_{k=0}^{N^{(0)}}{w_{ki}^{(1)}x_k})}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вычисление градиента начнем с выходного слоя.** \n",
    "\n",
    "Вспомним как вычисляется выход:\n",
    "\n",
    "$$ y_j^{(M)}=\\sigma_j^{(M)}(z_j^{(M)})$$\n",
    "$$z_j^{(M)} = \\sum_{i=0}^{N^{(M-1)}}{w_{ij}^{(M)}y_i^{(M-1)}}$$\n",
    "\n",
    "Будем последовательно вычислять:\n",
    "\n",
    "1. $\\boldsymbol{\\frac{\\partial E}{\\partial y_j^{(M)}}}.$\n",
    "Так как функция E нам известна, то мы можем вычислить частные производные этой функции по переменным \n",
    "$y_1^{(M)}, ..., y_{N^{(M)}}^{(M)}. $ \n",
    "Например, если\n",
    "$$E(\\boldsymbol{\\widehat{y}},  \\boldsymbol{y}^{(M)}) = \\frac{1}{2} \\| \\boldsymbol{\\widehat{y}} - \\boldsymbol{y}^{(M)} \\|_2^2 = \\frac{1}{2} \\sum_{j=1}^{N^{(M)}}{( \\widehat{y_j} - y_j^{(M)}) ^ 2},$$ \n",
    "то\n",
    "$$\\frac{\\partial E}{\\partial y_j^{(M)}} = y_j^{(M)} - \\widehat{y_j}$$\n",
    "Заметим, что в этом случае частная производная $E$ по $y_j^{(M)}$ равна ошибки на объекте $x_i$.\n",
    "\n",
    "2. $\\boldsymbol{\\frac{\\mathrm{\\partial E}}{\\partial z_j^{(M)}}}.$ \n",
    "Функция ошибки \n",
    "$E = E(y_1^{(M)}, ..., y_{N^{(M)}}^{(M)}) = E(y_1^{(M)}(z_1^{(M)}), ..., y_{N^{(M)}}^{(M)}(z_{N^{(M)}}^{(M)}))$.\n",
    "Вычислим \n",
    "$\\frac{\\mathrm{\\partial E}}{\\partial z_j^{(M)}}$ \n",
    "применив правило вычисление производной сложной функции \n",
    "$$\\frac{\\mathrm{\\partial}}{\\partial z_j^{(M)}} E = \\frac{\\partial E}{\\partial y_j^{(M)}} \\frac{\\partial y_j^{(M)}}{\\partial z_j^{(M)}} =  \\frac{\\partial E}{\\partial y_j^{(M)}} (\\sigma^{(M)}_j)' $$\n",
    "Важно, что $(\\sigma^{(M)}_j)'$ берется в точке \n",
    "$$z_j^{(M)}= \\sum_{i=0}^{N^{(M-1)}}{w_{ij}^{(M)}\\sigma_i^{(M-1)}(\\sum_{k=0}^{N^{(M-2)}}{w_{ki}^{(M-1)}\\sigma_k^{(M-2)}(...)})}.$$ \n",
    "Заметим что при прямом проходе, мы  вычисляем значение $z_j^{(M)}$, теперь дополнительно при прямом проходе будет сохранять это значение. Тогда вычисление \n",
    "$\\frac{\\mathrm{\\partial}}{\\partial z_i^{(M)}} E$ \n",
    "представляет собой вычисление значение  $(\\sigma^{(M)}_j)'$ в уже известной точке и перемножение двух чисел.\n",
    "\n",
    "3. $\\boldsymbol{\\frac{\\partial E}{\\partial w_{ij}^{(M)}}}.$ \n",
    "Функция ошибки \n",
    "$E = E(z_1^{(M)}, z_2^{(M)}, ..., z_{N^{(M)}}^{(M)})$. \n",
    "Вспомним, что \n",
    "$z_j^{(M)} = \\sum_{i=0}^{N^{(M-1)}}{w_{ij}^{(M)}y_i^{(M-1)}}$. \n",
    "Вес $w_{ij}^{(M)}$ входит только в одну сумму $z_j^{(M)}$. Тогда:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{ij}^{(M)}}= \\frac{\\partial E}{\\partial z_j^{(M)}} \\frac{\\partial z_j^{(M)}}{\\partial w_{ij}^{(M)}} =   \\frac{\\partial E}{\\partial z_j^{(M)}} \\frac{\\partial(\\sum_{k=0}^{N^{(M - 1)}}{w_{kj}^{(M)}y_k^{(M - 1)}}) }{\\partial w_{ij}^{(M)}} = \\frac{\\partial E}{\\partial z_j^{(M)}} y_i^{(M - 1)} $$\n",
    "\n",
    "Таким образом, мы  за 3 шага вычислили $\\frac{\\partial}{\\partial w_{ij}^{(n)}} E$ для последнего выходного слоя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вычисление градиента произвольного внутренного слоя**\n",
    "\n",
    "Рассмотрим произвольный внутренний слой n. Вспомним что выходы этого слоя $y_i^{(n)}$ связаны с $z_j^{(n + 1)}$ следующего слоя соотношением:\n",
    "$$z_j^{(n + 1)}=\\sum_{i=0}^{N_{n}}{w_{ij}^{(n + 1)}y_i^{(n)}}, \\quad  j= \\overline{1\\mathinner {\\ldotp \\ldotp}N^{(n+1)}}$$\n",
    "Можно сказать, что \n",
    "$E = E(z_1^{(n + 1)}, z_2^{(n + 1)},..., z_{N^{(n+1)}}^{(n + 1)})$. \n",
    "А производные \n",
    "$\\frac{\\partial E}{\\partial z_{j}^{(n + 1)}}$ \n",
    "были посчитаны на предыдущем шаге.\n",
    "\n",
    "Тогда для \n",
    "$\\boldsymbol{\\frac{\\partial E}{\\partial y_i^{(n)}}}$ \n",
    "$$\\frac{\\partial E}{\\partial y_i^{(n)}} = \\sum_{j=1}^{N^{(n+1)}}{\\frac{\\partial E}{\\partial z_{j}^{(n + 1)}}} \\frac{\\partial z_{j}^{(n + 1)}}{\\partial y_{i}^{(n)}} = \\sum_{j=1}^{N^{(n+1)}}{\\frac{\\partial E}{\\partial z_{j}^{(n + 1)}}} \\frac{\\partial (\\sum_{k=0}^{N_{n}}{w_{kj}^{(n + 1)}y_k^{(n)}})}{\\partial y_{i}^{(n)}} = \\sum_{j=1}^{N^{(n+1)}}{\\frac{\\partial E}{\\partial z_{j}^{(n + 1)}}} w_{ij}^{(n + 1)} $$\n",
    "\n",
    "Эту величину, по аналогии с последним слоем будем называть ошибкой сети на скрытом слое. Заметим, что \n",
    "$$\\frac{\\partial E}{\\partial y_i^{(n)}} = \\sum_{j=1}^{N^{(n+1)}}{\\frac{\\partial E}{\\partial z_{j}^{(n + 1)}}} w_{ij}^{(n + 1)} =  \\sum_{j=1}^{N^{(n+1)}}{\\frac{\\partial E}{\\partial y_{j}^{(n + 1)}}}(\\sigma^{(n + 1)}_j)'  w_{ij}^{(n + 1)}.$$\n",
    "Таким образом, мы вычисляем ошибку текущего слоя через ошибку предыдущего, распространяя ее \"задом наперед\". Отсюда и название алгоритма — обратное распространение ошибок.\n",
    "\n",
    "Вычисление\n",
    "$\\boldsymbol{\\frac{\\mathrm{\\partial E}}{\\partial z_i^{(n)}}}$ и $\\boldsymbol{\\frac{\\partial E}{\\partial w_{ij}^{(n)}}}$ выполняется абсолютно аналогично последнему слою.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, мы умеем последовательно вычислять частные производные по всем весам от последнего слоя к нейросети к первому. Заметим что все произодные можно быстро вычислять матрично."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическая часть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой части вам предстоит научиться:\n",
    "* вычислять производные среднеквадратичной функции ошибки и категориальной кросс энтропии\n",
    "* выполнять прямой и обратный проход неросети, состоящей из полносвязных слоей и функции антивации ReLu и Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1.** Вычисление производных функций ошибок\n",
    "\n",
    "Вам дан интерфейс класса `Loss`, вам нужно реализовать вычисление значение функции и ее градиента для среднеквадратичной ошибки $MSE$, а также для категориальной кросс-энтропии $H$. \n",
    "\n",
    "Пусть $\\widehat{y_i}$ - истинное значение функции, $y_i$ - предсказанное, тогда:\n",
    "$$MSE(\\boldsymbol{\\widehat{y}}, \\boldsymbol{y})=\\frac{1}{d}\\sum_{i=1}^{d}{( \\widehat{y_i} - y_i)^2}$$\n",
    "$$H(\\boldsymbol{\\widehat{y}}, \\boldsymbol{y})=-\\frac{1}{d}\\sum_{i=1}^{d}{\\widehat{y_i}\\log({y_i})}$$\n",
    "\n",
    "Для численной устойчивости вам предлагается также реализовать градиент связки `softax + crossentropy`(в функции `gradient_with_sofmax`). Преобразование $softmax$ над вектором $x=(x_1, ..., x_d)$ можно записать как \n",
    "$$y_i = \\frac{e^{x_i}}{\\sum_{j=0}^{d}{e^{x_j}}}$$\n",
    "Функция `gradient_with_sofmax` должна возвращать \n",
    "$(\\frac{\\partial E}{\\partial x_1}, ..., \\frac{\\partial E}{\\partial x_d})$\n",
    "(Вам необходимо выполнить аналитические преобразования над $\\frac{\\partial E}{\\partial x_i}$ так, чтобы аналичически он зависел только от $\\boldsymbol{\\widehat{y}}$ и $\\boldsymbol{y})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import base64\n",
    "import copy\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        y_true: np.array(d), ground truth (correct) labels\n",
    "        y_pred: np.array(d), estimated target values\n",
    "        ---\n",
    "        output: loss\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def gradient(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        y_true: np.array(d), ground truth (correct) labels\n",
    "        y_pred: np.array(d), estimated target values\n",
    "        ---\n",
    "        output: np.array(d), gradient loss to y_pred\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError(Loss):\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return ((y_true - y_pred) ** 2).sum() / y_true.size\n",
    "\n",
    "    def gradient(self, y_true, y_pred):\n",
    "        return (y_pred - y_true) * 2 / y_true.size\n",
    "\n",
    "\n",
    "class CategoricalCrossentropy(Loss):\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return -(y_true * np.log(y_pred)).sum() / y_true.size\n",
    "\n",
    "    def gradient(self, y_true, y_pred):\n",
    "        return -(y_true / y_pred) / y_true.size\n",
    "\n",
    "    def gradient_with_softmax(self, y_true, y_pred):\n",
    "        return (y_pred - y_true) / y_true.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедитесь в правильности работы ваших функций с помощью небольшого числа unit-тестов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_MSG = \"Error in test {}.\\n\\ty_true:{},\\n\\ty_pred:{},\\n\\tactual output:{}\\n\\tdesired output:{}\"\n",
    "\n",
    "def decode_answer(base64_string, shape):\n",
    "    buf = base64.decodebytes(base64_string)\n",
    "    return np.frombuffer(buf, dtype=np.float).reshape(shape)\n",
    "\n",
    "def check_answers(actual_values, desired_values, msg=''):\n",
    "    for i, (actual_value, desired_value) in enumerate(zip(actual_values, desired_values)):\n",
    "        msg = ERROR_MSG.format(i, Y_TRUE[i], Y_PRED[i], actual_value, desired_value)\n",
    "        np.testing.assert_almost_equal(actual_value, desired_value, err_msg=msg, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456)\n",
    "\n",
    "SHAPE = (10, 5)\n",
    "Y_TRUE = np.rint(np.random.random(size=SHAPE))\n",
    "Y_PRED = np.random.random(size=SHAPE)\n",
    "\n",
    "MSE_ERRORS = decode_answer(\n",
    "    b'I0+rUDZfsj+65BmGzuvTP+ITdyZtp9E/jovoXZbOxj+tM4rNGfjRP8WqZPINQ7o/qkWFqWJrvj/YygOu3+7iP2xXHf8FLtc/nh8u'\n",
    "    b'JeH90z8=', shape=SHAPE[0]\n",
    ")\n",
    "\n",
    "MSE_GRADIENTS = decode_answer(\n",
    "    b'bbbcKhRtpD9tOdGaZi27v2ZYmkJFG3s//n322ZiNyr8m6+LE9aWjP6AI7iZLdsk/C3BaoCJ51T/NU4CxsQalvyCAv50cwdI/Jnla'\n",
    "    b'YuW0uL+ge/FUpbTMP8oHwWfBI8g/078UJnJrrD+agsQdii7Xv/P1Wpvcppe/ALOZhal2qz/J0z1DWiXQPy69kl9AdMK/KyUC2VwB'\n",
    "    b'zb+TKsfV3+qvP61ZjQQ/I6y/861aiGl80T9aw6ZZnY2hP05TbFQuN8a/pf7fNLXD1b91mBtGF3LIP2BQLEj8766/QK1QkZqzrD8A'\n",
    "    b'Z3EKfd/APyILN4pDvMK/IkWU2OF8w7/dKXoh5znEP2Op3oXyxcC/bf/5e0B1s7+ti6ZZedDDP8APmQhD5Ms/m5mMtC1R2T9T4D8Z'\n",
    "    b'LzfZPy+LSRFEHtC/gzVVrZfSzD+G39szoJO5P2acoFCBSoO/d+LLLFDn1D+yZblH9XbYv5OyW97C+8Q/nCtIKLzV2L9dGTmp/Em1'\n",
    "    b'P7SuSAYBJtA//Z/XUtyLwb+9hvZzq4e5Pw==', shape=SHAPE\n",
    ")\n",
    "CROSS_ENTROPY_ERRORS = decode_answer(\n",
    "    b'Lk/El56cyj/WBoxO/q6zP0unis259t4/pUfrxKNr0D96LACShLzgPwpVpmwNtr8/yNI87LnRyz93hYk3LG3JP09lbzfFF+Q/stF9'\n",
    "    b'Vtkm6T8=', shape=(SHAPE[0])\n",
    ")\n",
    "CROSS_ENTROPY_GRADIENTS = decode_answer(\n",
    "    b'AAAAAAAAAICQTFUt2sf1vwAAAAAAAACAmWpZOGWeAMAAAAAAAAAAgAAAAAAAAACAAAAAAAAAAICCQnznotTxvwAAAAAAAACAXyHI'\n",
    "    b'8ogW9b8AAAAAAAAAgAAAAAAAAACAAAAAAAAAAID9kuL7NywlwN9kei0D+/C/AAAAAAAAAIAAAAAAAAAAgNm59BtSBPm/wPXRt3J0'\n",
    "    b'AsAAAAAAAAAAgDhJMLFijPK/AAAAAAAAAIAAAAAAAAAAgIi4e0BwQ/y/9c0zp6WyGsAAAAAAAAAAgHD+3KnZ2PK/AAAAAAAAAIAA'\n",
    "    b'AAAAAAAAgFqtaQ7QO/m/QoDlrBTV+b8AAAAAAAAAgAhQB8ahy/e/DnZxuenA878AAAAAAAAAgAAAAAAAAACAAAAAAAAAAIAAAAAA'\n",
    "    b'AAAAgHRxm/JtmQXAAAAAAAAAAIAAAAAAAAAAgKLsIAbIYvC/AAAAAAAAAIDiZmdoeIw2wAAAAAAAAACA5l+CC9q6QMAAAAAAAAAA'\n",
    "    b'gAAAAAAAAACAtRNb8pFX+L8AAAAAAAAAgA==', shape=SHAPE\n",
    ")\n",
    "CROSS_ENTRY_GRADIENTS_WITH_SOFTMAX = decode_answer(\n",
    "    b'COSTNVmIuT/kw8IgYPzQv0B3oEkL8ZA/vw46iH+Y4L/wpRs2c4+4P8iKqfDd098/DgxxSGvX6j/AaOAdXki6vyhgL8Vjcec/cBfx'\n",
    "    b'uh7izr9E7RZV5/DhP7xJscGxLN4/5PfMVyfDwT9BozWlLPrsv3CzMcKTkK2/4A+A8ykqwT+7SA3UsC7kP3psd3dQEde/O1ehB9og'\n",
    "    b'4r+cepzly/LDPwxY2GIHlsG/cFlx6oPb5T8wdBCwBPG1PyJoh+n5xNu/Tv4XgqI067+SfqIXnY7ePzyyG639VcO/SGzSmkDwwT/A'\n",
    "    b'wA1NXBfVP+rNxGxUa9e/ala5Thpc2L9UtNjpYEjZP7xTVidv99S/SH/4mpBSyL+YLhCwl8TYP9ipX+WpbuE/AsCvIXml7z9o2I/f'\n",
    "    b'+oTvP/vtmxXVJeS/ckFVzJ4D4j9o19JAiPjPP4DDyKQhHZi/Fdv+NyQh6j8ev6eZspTuvzif8pWzOto/gzZaMisL77+0X4fTe5zK'\n",
    "    b'P2Ha2keBL+Q//IeNZ9Pu1b9sKPRQlunPPw==', shape=SHAPE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not almost equal to 7 decimals\nError in test 0.\n\ty_true:[0. 1. 0. 1. 0.],\n\ty_pred:[0.09973676 0.73459622 0.01654451 0.4813845  0.09593887],\n\tactual output:[-0.         -1.36129205 -0.         -2.0773415  -0.        ]\n\tdesired output:[-0.         -0.27225841 -0.         -0.4154683  -0.        ]\n(mismatch 40.0%)\n x: array([-0.       , -1.3612921, -0.       , -2.0773415, -0.       ])\n y: array([-0.       , -0.2722584, -0.       , -0.4154683, -0.       ])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-2994c2c01c3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mcross_entropy_gradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCategoricalCrossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_TRUE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_PRED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mcheck_answers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCROSS_ENTROPY_GRADIENTS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcross_entropy_gradients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m cross_entropy_grad_softmax = [CategoricalCrossentropy().gradient_with_softmax(y_true, y_pred) \n",
      "\u001b[1;32m<ipython-input-13-d931b1217bd7>\u001b[0m in \u001b[0;36mcheck_answers\u001b[1;34m(actual_values, desired_values, msg)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mactual_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesired_value\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesired_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mERROR_MSG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_TRUE\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_PRED\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactual_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesired_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_almost_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesired_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tens\\lib\\site-packages\\numpy\\testing\\_private\\utils.py\u001b[0m in \u001b[0;36massert_almost_equal\u001b[1;34m(actual, desired, decimal, err_msg, verbose)\u001b[0m\n\u001b[0;32m    566\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m             \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdesired\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0massert_array_almost_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesired\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# If one of desired/actual is not finite, handle it specially here:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tens\\lib\\site-packages\\numpy\\testing\\_private\\utils.py\u001b[0m in \u001b[0;36massert_array_almost_equal\u001b[1;34m(x, y, decimal, err_msg, verbose)\u001b[0m\n\u001b[0;32m    971\u001b[0m     assert_array_compare(compare, x, y, err_msg=err_msg, verbose=verbose,\n\u001b[0;32m    972\u001b[0m              \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Arrays are not almost equal to %d decimals'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m              precision=decimal)\n\u001b[0m\u001b[0;32m    974\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tens\\lib\\site-packages\\numpy\\testing\\_private\\utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[1;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[0;32m    787\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[1;32m--> 789\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    790\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: \nArrays are not almost equal to 7 decimals\nError in test 0.\n\ty_true:[0. 1. 0. 1. 0.],\n\ty_pred:[0.09973676 0.73459622 0.01654451 0.4813845  0.09593887],\n\tactual output:[-0.         -1.36129205 -0.         -2.0773415  -0.        ]\n\tdesired output:[-0.         -0.27225841 -0.         -0.4154683  -0.        ]\n(mismatch 40.0%)\n x: array([-0.       , -1.3612921, -0.       , -2.0773415, -0.       ])\n y: array([-0.       , -0.2722584, -0.       , -0.4154683, -0.       ])"
     ]
    }
   ],
   "source": [
    "mse_errors = [MeanSquaredError()(y_true, y_pred) for y_true, y_pred in zip(Y_TRUE, Y_PRED)]\n",
    "check_answers(MSE_ERRORS, mse_errors)\n",
    "\n",
    "mse_gradients = [MeanSquaredError().gradient(y_true, y_pred) for y_true, y_pred in zip(Y_TRUE, Y_PRED)]\n",
    "check_answers(MSE_GRADIENTS, mse_gradients)\n",
    "\n",
    "cross_entropy_errors = [CategoricalCrossentropy()(y_true, y_pred) for y_true, y_pred in zip(Y_TRUE, Y_PRED)]\n",
    "check_answers(CROSS_ENTROPY_ERRORS, cross_entropy_errors)\n",
    "\n",
    "cross_entropy_gradients = [CategoricalCrossentropy().gradient(y_true, y_pred) for y_true, y_pred in zip(Y_TRUE, Y_PRED)]\n",
    "check_answers(CROSS_ENTROPY_GRADIENTS, cross_entropy_gradients)\n",
    "\n",
    "cross_entropy_grad_softmax = [CategoricalCrossentropy().gradient_with_softmax(y_true, y_pred) \n",
    "                              for y_true, y_pred in zip(Y_TRUE, Y_PRED)]\n",
    "check_answers(CROSS_ENTRY_GRADIENTS_WITH_SOFTMAX, cross_entropy_grad_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2.** Выполнение прямого и обратного прохода нейросети\n",
    "\n",
    "В этой части задания вам будет необходимо реализовать forward и backward проходы для 3-х типов слоев:\n",
    "* `Linear`, выполняет линейную комбинацию входов с весами $y_j=\\sum_{i=0}^{N}{w_{ij}x_i}, j=\\overline{1 {\\ldotp \\ldotp}M}$\n",
    "* `ReLu`, нелинейная активация $y_j^{(n)}=\\max(x_j), j=\\overline{1{\\ldotp \\ldotp}M}$\n",
    "* `Softmax`, $y_j = \\frac{e^{x_j}}{\\sum_{i=0}^{d}{e^{x_i}}}, j=\\overline{1{\\ldotp \\ldotp}M}$\n",
    "\n",
    "Вам дан шаблон нейросетевой модели, которая умееет последовательно добавлять слои друг за другом. Сейчас вам стоит обратит внимание только на функции `__init__`, `add`, `forward` и `backward`. Пока при инициализации модели параметр `optimizer` оставляйте равным None. Также вам дан абстракный класс `Layer`, который предоставляет интерфейс одного слоя нейросети. Вам необходимо реализовать функции `_forward` и `_backward`. Все вычисления необходимо делать матрично.\n",
    "\n",
    "Замечание:\n",
    "    обратите внимание на функцию `_build` слоя `Linear`, веса соответвущие bias'ам добавляются последним столбцом.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, loss=None, optimizer=None):\n",
    "        self._layers = []\n",
    "        self._loss = loss\n",
    "        self._optimizer = optimizer\n",
    "        self._outputs = None\n",
    "\n",
    "    def add(self, layer):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if not self._layers:\n",
    "            layer.build(optimizer=self._optimizer)\n",
    "        else:\n",
    "            layer.build(self._layers[-1], optimizer=self._optimizer)\n",
    "        self._layers.append(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = inputs\n",
    "        for layer in self._layers:\n",
    "            outputs = layer.forward(outputs)\n",
    "        self._outputs = outputs\n",
    "        return self._outputs\n",
    "\n",
    "    def backward(self, outputs, use_gradient_softmax_with_loss=False):\n",
    "        if self._loss is None:\n",
    "            raise ValueError(\"Loss is not defined\")\n",
    "\n",
    "        if use_gradient_softmax_with_loss:\n",
    "            grad_outputs = [self._loss.gradient_with_softmax(outputs[i], self._outputs[i])\n",
    "                               for i in range(outputs.shape[0])]\n",
    "            backward_layers = self._layers[:-1]\n",
    "        else:\n",
    "            grad_outputs = [self._loss.gradient(outputs[i], self._outputs[i])\n",
    "                               for i in range(outputs.shape[0])]\n",
    "            backward_layers = self._layers\n",
    "\n",
    "        grad_outputs = np.array(grad_outputs)\n",
    "        for layer in backward_layers[::-1]:\n",
    "            grad_outputs = layer.backward(grad_outputs)\n",
    "\n",
    "    def update_weights(self, x_batch, y_batch, use_gradient_softmax_with_loss=False):\n",
    "        if self._optimizer is None:\n",
    "            raise ValueError(\"Optimizer is not defined\")\n",
    "        self.forward(x_batch)\n",
    "        self.backward(y_batch, use_gradient_softmax_with_loss)\n",
    "        for layer in self._layers[::-1]:\n",
    "            layer.update_weights()\n",
    "\n",
    "    def fit(self, X, Y, batch_size, epochs, shuffle=True, X_val=None, Y_val=None, use_gradient_softmax_with_loss=False):\n",
    "        size = X.shape[0]\n",
    "        X_train, y_train = X[:], Y[:]\n",
    "\n",
    "        self.loss_train_history = []\n",
    "        self.loss_val_history = []\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            if shuffle:\n",
    "                p = np.random.permutation(size)\n",
    "                X_train, y_train = X[p], Y[p]\n",
    "            for step in range(size // batch_size):\n",
    "                ind_slice = slice(step * batch_size, (step + 1) * batch_size)\n",
    "                self.update_weights(X_train[ind_slice], y_train[ind_slice], use_gradient_softmax_with_loss)\n",
    "            train_loss = self.evaluate(X_train, y_train, batch_size)\n",
    "\n",
    "            if (X_val is not None) and (Y_val is not None):\n",
    "                val_loss = self.evaluate(X_val, Y_val, batch_size)\n",
    "                self.loss_val_history.append(val_loss)\n",
    "                self.loss_train_history.append(train_loss)\n",
    "                print(\"Epoch: {:d}, train loss: {:f}, val loss: {:f}\".format(epoch, train_loss, val_loss))\n",
    "            else:\n",
    "                self.loss_train_history.append(train_loss)\n",
    "                print(\"Epoch: {:d}, train loss: {:f}\".format(epoch, train_loss))\n",
    "\n",
    "    def evaluate(self, X, Y, batch_size):\n",
    "        if self._loss is None:\n",
    "            raise ValueError(\"Loss is not defined\")\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise ValueError(\"X and Y must have equal size\")\n",
    "\n",
    "        Y_pred = np.empty(Y.shape)\n",
    "        size = X.shape[0]\n",
    "        for step in range(size // batch_size + 1):\n",
    "            ind_slice = slice(step * batch_size, (step + 1) * batch_size)\n",
    "            Y_pred[ind_slice] = self.forward(X[ind_slice])\n",
    "        losses = [self._loss(Y[i], Y_pred[i]) for i in range(size)]\n",
    "        return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer(object):\n",
    "    def __call__(self, shape):\n",
    "        n = shape[0]\n",
    "        return np.random.randn(*shape) * np.sqrt(2.0/n)\n",
    "\n",
    "\n",
    "class ZerosInitializer(object):\n",
    "    def __call__(self, shape):\n",
    "        return np.zeros(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(abc.ABC):\n",
    "    def __init__(self, input_dim=None):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = None\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.d_inputs = None\n",
    "        self.d_outputs = None\n",
    "        self._optimizer = None\n",
    "\n",
    "        self._is_build = False\n",
    "\n",
    "    def build(self, prev_layer=None, optimizer=None):\n",
    "        self._optimizer = copy.deepcopy(optimizer)\n",
    "        if prev_layer is not None:\n",
    "            self.input_dim = prev_layer.output_dim\n",
    "        elif self.input_dim is None:\n",
    "            raise ValueError('Input dimension is not determine.'\n",
    "                             'If this first layer, please, use param \"input_dim\"')\n",
    "        self._is_build = True\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if not self._is_build:\n",
    "            raise ValueError(\"Layer is not build\")\n",
    "        if inputs.shape[1:] != (self.input_dim,):\n",
    "            raise ValueError(\"Input shape is not correct\")\n",
    "        return self._forward(inputs)\n",
    "\n",
    "    def backward(self, grad_outputs):\n",
    "        if self.inputs is None:\n",
    "            raise ValueError(\"Forward pass is not performed\")\n",
    "        return self._backward(grad_outputs)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: np.array((n, d)), input values, n - batch size, d - number input features\n",
    "        ---\n",
    "        output: np.array((n, c)), output values, n - batch size, c - number output features\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _backward(self, grad_outputs):\n",
    "        \"\"\"\n",
    "        grad_outputs: np.array((n, c)), gradient by outputs,\n",
    "                      n - batch size, c - number output features of this layer\n",
    "        ---\n",
    "        output: np.array((n, d)), gradient by inputs,\n",
    "                n - batch size,  d - number input features of this layer\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def update_weights(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, units, input_dim=None,\n",
    "                 weights_initializer=None, bias_initializer=None):\n",
    "        super().__init__(input_dim)\n",
    "        self.output_dim = units\n",
    "\n",
    "        self.weights = None\n",
    "        self.mean_d_weights = None # mean value gradient weights by batch\n",
    "\n",
    "        self._weights_initializer = weights_initializer if weights_initializer else HeInitializer()\n",
    "        self._bias_initializer = bias_initializer if bias_initializer else ZerosInitializer()\n",
    "\n",
    "    def build(self, prev_layer=None, optimizer=None):\n",
    "        super().build(prev_layer, optimizer)\n",
    "        weights = self._weights_initializer((self.input_dim, self.output_dim))\n",
    "        bias = self._bias_initializer((1, self.output_dim))\n",
    "        self.weights = np.vstack((weights, bias))\n",
    "\n",
    "    def _forward(self, inputs):\n",
    "        self.inputs = inputs.copy()\n",
    "        self.outputs = np.dot(np.hstack((inputs, np.ones((inputs.shape[0],1)))), self.weights)\n",
    "        return self.outputs\n",
    "\n",
    "    def _backward(self, grad_outputs):\n",
    "        self.d_outputs = grad_outputs.copy()\n",
    "        self.d_inputs = np.dot(grad_outputs, np.transpose(self.weights[:-1]))\n",
    "        self.mean_d_weights = np.zeros_like(self.weights)\n",
    "        n = grad_outputs.shape[0]\n",
    "        for i in range(n):\n",
    "            self.mean_d_weights[:-1] += np.dot(self.inputs[i].reshape(-1, 1), grad_outputs[i].reshape(1, -1)) / n\n",
    "        self.mean_d_weights[-1] = np.mean(grad_outputs, axis=0)\n",
    "        return self.d_inputs\n",
    "\n",
    "    def update_weights(self):\n",
    "        self.weights = self._optimizer.update_weights(self.weights, self.mean_d_weights)\n",
    "        pass\n",
    "\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self, input_dim=None):\n",
    "        super().__init__(input_dim)\n",
    "\n",
    "    def build(self, prev_layer=None, optimizer=None):\n",
    "        super().build(prev_layer, optimizer)\n",
    "        self.output_dim = self.input_dim\n",
    "\n",
    "    def _forward(self, inputs):\n",
    "        self.inputs = inputs.copy()\n",
    "        self.outputs = np.maximum(inputs, np.zeros_like(inputs))\n",
    "        return self.outputs\n",
    "\n",
    "    def _backward(self, grad_outputs):\n",
    "        self.d_outputs = grad_outputs.copy()\n",
    "        derivative = np.maximum(np.sign(self.inputs), np.zeros_like(self.inputs))\n",
    "        self.d_inputs = grad_outputs * derivative\n",
    "        return self.d_inputs\n",
    "\n",
    "    def update_weights(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Softmax(Layer):\n",
    "    def __init__(self, input_dim=None):\n",
    "        super().__init__(input_dim)\n",
    "\n",
    "    def build(self, prev_layer=None, optimizer=None):\n",
    "        super().build(prev_layer, optimizer)\n",
    "        self.output_dim = self.input_dim\n",
    "\n",
    "    def _forward(self, inputs):\n",
    "        self.inputs = inputs.copy()\n",
    "        self.outputs = np.exp(inputs)\n",
    "        self.outputs /= np.tile(self.outputs.sum(axis=1).reshape(-1, 1), (1, inputs.shape[1]))\n",
    "        return self.outputs\n",
    "\n",
    "    def _backward(self, grad_outputs):\n",
    "        self.d_outputs = grad_outputs.copy()\n",
    "        self.d_inputs = np.empty_like(grad_outputs)\n",
    "        for i in range(grad_outputs.shape[0]):\n",
    "            d_matrix = -np.dot(self.outputs[i].reshape(-1, 1), self.outputs[i].reshape(1, -1))\n",
    "            d_matrix[np.identity(grad_outputs.shape[1], bool)] += self.outputs[i]\n",
    "            self.d_inputs[i] = np.dot(d_matrix, grad_outputs[i])\n",
    "        return self.d_inputs\n",
    "\n",
    "    def update_weights(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедитесь в правильности реализации вами функций с помощью небольшого числа unit-тестов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_answer(base64_string, shape):\n",
    "    buf = base64.decodebytes(base64_string)\n",
    "    return np.frombuffer(buf, dtype=np.float).reshape(shape)\n",
    "\n",
    "np.random.seed(123456)\n",
    "\n",
    "INPUT_DIM, OUTPUT_DIM = 4, 6\n",
    "BATCH_SIZE = 3\n",
    "COUNT_TESTS = 10\n",
    "\n",
    "INPUTS = np.random.normal(size=(COUNT_TESTS, BATCH_SIZE, INPUT_DIM))\n",
    "WEIGHTS = np.random.normal(size=(COUNT_TESTS, INPUT_DIM + 1, OUTPUT_DIM))\n",
    "LINEAR_D_OUTPUTS = np.random.normal(size=(COUNT_TESTS, BATCH_SIZE, OUTPUT_DIM))\n",
    "RELU_D_OUTPUTS = np.random.normal(size=(COUNT_TESTS, BATCH_SIZE, INPUT_DIM))\n",
    "SOFTMAX_D_OUTPUTS = np.random.normal(size=(COUNT_TESTS, BATCH_SIZE, INPUT_DIM))\n",
    "\n",
    "LINEAR_OUTPUTS = decode_answer(\n",
    "    b'wGm3fs21079DBbbsam/fv0YsX4hX9/o/Nxh4K63n8T++UomsDXPrvxbi2hKA8gjAl03DtDo1AcB05j4D6Bvcv1d2xALzbwBADAx/'\n",
    "    b'H0G0AkD31QxMgaX0P+gv0xPBnfK//f64pm5VBUDLFwHUBV4EwAiYgNzWLA3Ax4deeVK9AsDYJpe26kwEQGVn2t7ZWwLA/HC9xMss'\n",
    "    b'9L+IeOKWVevZv5Rfd295DbI/WHvyPVAW979E6GKRpFPmP8x4zXNcUwTAqCRqOrLM8T/151+zeYr+vx6bEJiWGNC/mhc6p5SqAMCh'\n",
    "    b'SuMGWLDcv24FuPkfPeU//mbyah5d9r+ucwIVIU3oP81dEoOegQVAyKhCIITQDcAwTKAcRY4NQPjHLQNXwALAestiiX2EBkDemn7g'\n",
    "    b'j8DwvzBqu0YGnPw/wNLLm4gz8r8MoZpd8svwv/70sbu0xP2/4Z0Nh94+AMAow1b45oIKwMdHYH/MvwrA506PZHXj4z9aLm5Z2+3s'\n",
    "    b'P8Tk/paM3gZAqCclSm3M+z9pzl5fkHIYQBkKiIdTjRJAkWkBGd358D/IbqQCKN77v7JS+XYenvC/Bo9V1xLnEEAkelfNQC3kPwGY'\n",
    "    b'/nu93/8/sE1Qipm89L/QNoNnBFqkv0TCcC86m+I/tLP+xucZ0j8QA6VJV2HiPyc1Ru5nFgJA5efyWSarA0Cc5Fqc2qrNv2vZQdXk'\n",
    "    b'sQDALOJVE8sv/j/mrk0EYifjP+4ZCcQaJPc/V0d//SUe5b+gPNHw9VzSP/hOIius5gTANiVWAomHFcD7wcLEf3Dwv8EPRV+UIBlA'\n",
    "    b'KHyQtLH1+T9gr+67iXUGQHyz3s0wTNE/1vmK3JcAD8AxuOL3pTL4vwVlfP0a6xdAKk4O0RKRE8DzJk+ETJP1Pwml9+piFgrAsEbz'\n",
    "    b'ANbAC8AYXacpBbXOvxRlJ9bEeOo/yaILuC6LCsDiNBbTRO7Vv6RKEPT8Avu/2uPPVplQ0j+jPgFnam6xvyGX5u0PhPC/X2dnCYQR'\n",
    "    b'EsCgGkfU3kHgvxQ0Am3FNss/V6Y63wHpC8A7tmYislPcPxQ+0M8ZYhFAQIPzNBpF+r9Yv2NY7SoQwOyBsYbZNvI/qLCJjvaL/D9S'\n",
    "    b'O4Mv3CS1vyftMo9OnAvAhP5pHLgYFcAg3GyiFC/pP+R7Q28JJ9C/p96cZCbV8z8pY0LfW6vpv2tGIwTKyxFAmtN1h/cK5T/uR0Ff'\n",
    "    b'LpEEwIOjB57uAfw/iH3ZhwYzB0BipUya/4/pPyz9FjDi5hdARbRzUdzv7j+YYMxaOSILQHANapLprda/IlSg8sQ28D8xDBK2gxnm'\n",
    "    b'P5CW0s+gnbY/gDZeg85Czj9SuLWI8vwLwHZfxhQpCN2/ir6FkxvEEsCShlKitBX2v9rq22/+hvO/QiehO1HX0j/eww3Ft83nP6Vs'\n",
    "    b'dgmGtAZAlP8lO1X99D9QydHZ4nXvPwZiWA6Ssea/TJJIWptA9r/uWjqs06v+v9An3PViyO6/KrQViu7RCcA+Kh7hTMu/v2zoa2ZW'\n",
    "    b'jBXAHrrgqgSM/j8gvM4nNqT3v4xtpLcZYRdAfoxz/N45DsA80sT+DY3yP15FUTkjIPo/oLiF9fzN9T94wKgxQHy9P4DTllkw19O/'\n",
    "    b'MtKPpb+g0b9yPM4rr97iv6xle4TvzP2/0gweWXoCDEBU3w4wKx30P9hmJE5kSAbAJkOcNM34G0BNIxfvXRcTwKb78y1erPe/lJ/Y'\n",
    "    b'redW+L9e47yfGZn1P0ZEwUoRiQHAzIFyAjDGAUC8bw0idVf2vyJwOBRMmAFAL+NuZOhDC8D+5VZNXyoJQLCicm3ysPW/SB7Coj/M'\n",
    "    b'2T8sjM1DuEzrv+rph7ycMhFAPyaGKEi3E8A02vtrHaXqP6L4cDNZe+y/c/Ys2fxu5r8nf4qpwmcSQIi0Q9ysMPI/Fh10yO1sDUAG'\n",
    "    b'6RLr7PUDwG5OmwH+59o/',\n",
    "    shape=(COUNT_TESTS, BATCH_SIZE, OUTPUT_DIM)\n",
    ")\n",
    "LINEAR_D_INPUTS = decode_answer(\n",
    "    b'P2edUgWR+r/bzKvlk3bjP9szpDHiifi/LYE0nMt2779h0J1t7RAMwPm13hRgIvo//0PlFKvn078mY9lXhU7dvzHH2LklFRVAj3BH'\n",
    "    b'TpX7AUDEabx1uEIHwBJTpAf2u9k/2UASQZeD0b8ZiUrHtnD5PzUUC+5tqgHAhKA4PIjR8L98KhExIHv1vyfDZ2OKQPy/z8zIh0tu'\n",
    "    b'DsBRGTYBOEEJwLQRGDDCDRHAq91D5VKlIMD95gLQO6QaQI8ES0rhliJAfZgckVxv9T9G/aTxuLrFvwv4niwXcfg//CNiqtSf67+s'\n",
    "    b'xePaogPqP/8f44x7EwzATjSTlQa21z+fmux6IjwPQGMvrv8zShHAbJQRvmQGAcBPLH4HSQ72vxmsITDMDiNAlDrNEjW99D9csOJ6'\n",
    "    b't7T+P93HoYdoKfM/7k3sVZ6J9r/6iK3y2S7ZP/l1IQvc7QTAFTesB+Nb9L9kIG01nPj/v++DG90/aeA/bOFczA0HAMBKeBXJECcV'\n",
    "    b'QEJNe+3vpBvAnwLL8efe4D+bRDTq7ezfv21YsH6B+vM/L6CZoBR21D8hMDDp4yT8v6XULQufIPY/s7GQkbha+b8exktRol0VQAQM'\n",
    "    b'qBQ/ZeI/qeq5APRYjb+lhC8HfqQCwBQiLbLre/u/0xv/24Gi3r801ygQjJv1P2KwMlh6H/u/NUi9uPw/8z8t3X2eBLfzPxLxpw5f'\n",
    "    b'N+c/RyP/A/ah4j+EobO9K38RwPXhyZcCAQFAVmb/UNjVB8Cd/+qN1o0OwK8yktnHcAjALgt4XYrD+L93CbzwJHv7v/0mhN5VO+u/'\n",
    "    b'SzTIYQ0H4j8Mc01xk1QPQFeOI+NWUQXAsP1mWBWl4r9uhE6QjNMEwGcPimeXce6/yI1g6owq8j9edpTiY4H9v1VBS0rUMwHA3wpn'\n",
    "    b'JKiM+b+gmZUgyMLWP4WXYJ9+lw9A5fhvmrbi5T8tCSJlLwIeQJrtuTutefk/4bkQwFPWEsAgAwRhO+QLwHwagU32TAHA4SMOiYKI'\n",
    "    b'7L+6FngpMMgBQDjYeTf1/Ok/4+2b7LP1+b+gXzZ1qKgMQBElUKRe0tW/cOWiD+H+9T/QarjGuP/8PzmlXoBeww3AIHsffOvw5b9P'\n",
    "    b'4Y9kZCcKwFfPnIHSDwtAwb567vp4wz9AImNVabrQv1ilXYAKTua/yRTFuDpc0b8Rxpcdo6QLQFhZ/XLpVQfAzrFbKSnx8b/4OfW8'\n",
    "    b'ovQFwL295MDX1Oo/pTQXX+ZsAMAKax04xwvwv8jcCidyEgLAYlrJn2I4CUBWSMjMKagOwAkewz9Jnum/',\n",
    "    shape=(COUNT_TESTS,  BATCH_SIZE, INPUT_DIM)\n",
    ")\n",
    "LINEAR_MEAD_D_WEIGHTS = decode_answer(\n",
    "    b'arfqHIAf479QsLlR0m7Wv+jN7ldX6+y/BpChF3zO4r+iO2bPKy/kv/2bdU6oisS/DiFD39Ma1L99QrcdZLrNv0KzC2HEb/K/n35H'\n",
    "    b'WJ2J3b83YGFa7N7kPyNAC1DDoqS/+CC4vvv+zj9Das+5yKinv5tyGRSeGMI/OWWympzx07+Ftb6M3CfgPxMqUKMHYOk/Nb1MLbVm'\n",
    "    b'6D+MCqeZckvXP0VSDrTOPvM/QNo7F1dP4D9TTSLALTPmPxnZ6L6n8uE/3Eoyzo780r+fv7SODo61v9CWDJI137k/4DEvqKkblb+7'\n",
    "    b'URIg42vxv2kD6okB3NS/1AllJjKQ6z98C8dIGr95P81bRcd3OMa/BFQe+lkr4L+/6OnoQdOjPzF9zjIkkK4/FIdNlRa5yb8VCkvn'\n",
    "    b'xhHBv9Uf8azEtL8/ynoNTAMDoj9/VXgneSTDv6jl+qMRrsq/d2DyQa0S+j8ZYBX9Kd3Wv3PPiSI6+bC/hABDe4iF67+1RrhSNQnX'\n",
    "    b'v4wdMNwj5tW/7ADbBBAx47+VPRR0msPVPzEC2dIDlrO/S3U26I2L6T9w8uvHkfbTP9VwiTbq7+A/J46sCQnu8L8r2SLRrSZ8v26W'\n",
    "    b'ATJ6fMM/i2TxxobCsL8doERHZ/aiP8NirrEgmtC/Y/YOGLrk5b+tous4Fn/Tv4hZTNPnst6/QTwB4nVetb+/ehcpxL3HP/EEdNKq'\n",
    "    b'lKs/aCawyDkg2T9VvYpYnG6nvw9Oou01DM0/B26MkVIuwj9VcuoOy1/wv+aK6ClOqdS/GZdYPeuC7L9PJw3O2YPnv0yt1binSPW/'\n",
    "    b'AvgQvByj47/p2QORQK/bP5kxgHJOquq/Vdq8oh/F6j+j8vUqhWvaPzIRwuMfwOA/DCCeqHZohj99+R1SI2S/PwoU1KtTfLC/YGIZ'\n",
    "    b'rKge77/DRA+HjlO3v9P2TkRFW9O/GBq7h6bRqD9b951ij0jtP0vtuEvb9uc/EXFrNq2E0D+D5QuRJ7LaP4OWI2iSLtK/8VhmW9yj'\n",
    "    b'6z+dFaRn5u7YP/PEW9U1tdy/bPa3t4Bz078x2BNPJKPRv4tp66+QyMc/SVuo3FgN1L8J3jF1nFGaP3TnWOkLt+c/2p2TlN2b5L8o'\n",
    "    b'wi8r7ofUvxs0iRHaQuQ/91Qlr9zt5L8MdWywPQGvv7Qm8jXTfec/Yz3Vr3y41r9ZP+2CQdbEv8rYDpBmOqA/l1NCINeT0z8JH8QE'\n",
    "    b'xXXgP76jPbRvg/M/oSJ5KdU81D8VQK3fVCyeP2vMKOhM56U/BZzrQA065L+Y7tDchoDmv0octzYo2PK/09Tl2se20L+VR9p6w6ng'\n",
    "    b'Py97+8+YdO8/GY2Ne+aDxT//8cj2z4qnvyWuZEad4fU//Jk/MSbXxL+8SsKLs3LgPxx0RhYjCu8/rOc0IsdnuT9Il9BhEieov3dg'\n",
    "    b'rP6KYPM/32QVYgu+0z+tURTx01/evxOE+J83UOm/SOaIvyUquL98UvdfsyO1P3Djfkny2/a/b+oz3DbMwb/U+UYpwcjVP3wfVR7E'\n",
    "    b'0Hk/odno8Hwq4r9XLF9XhDrSv6mFfNKLPPU/8HTl7HDNqr8P80KavPfePzXrvspu5+k/cQNiV0l3tb+biTHuj2u2v/MKMn9aIPE/'\n",
    "    b'i4Sjw58d5z8JlSifqwagv3cTV0MnJsm/zwNNPCVt8z/4vZrAbNHVP+gMHM9KvNo/KaoVfEMGAcCPuG9bwtziv8QXBlKdu+4/QzGT'\n",
    "    b'GegB+b9lyoyc+1XoP+8L5L0RquO/ud2z2b/s8D8eEsYVvIjiP1vPVU+/Cey/vFGN6Z9Y5r9vdHeb1az1v9H53gnWF72/gAvQjsoI'\n",
    "    b'AMCPnUx6VQnnv+NlhZ7DnPE/NC7dg5i/5r/HnktUju3zP3f4u/xaCde//W+EBL/C8D9h+aILAbbgP0OyioqCqey/WWjYgB0R5r9z'\n",
    "    b'c30eDzz0vwTjxye+sbq/ShLZt0cwsz8ovJGv+SHkP4Vl9csAA7C/rpcmJq4Z5b+EfET8lCvHP26w3mw9f4S/M9Cb/myByr+XP+IA'\n",
    "    b'c4XevwmdPXKelpo/3aoZilJwzj/5E1EgvB7Avwkbzzl1q6u/G4Wj3MoP5T+FnN3skvfmP8XZHeUb/ZM/59vxTkuRwb8jL7TU7aTR'\n",
    "    b'P1XrcMOdsrk/aWPI3zX0tb+3s6jTXT3Yv9mqBKsh2K0/uJXE+4YR6b9MJvsDFPbLP/Gx8J+Vdte/mX7Xh41b3T/D+uSIDyTlv4DA'\n",
    "    b'qy2aRcQ/tdjMwp6p7j/ldmmKBnqhvxVipc8j1Y8/kY6ZUT/82j90Z6cf+vKfv/0jJMafAtg/t8+tW7Tbvj+454FNAAimv1kdWjA2'\n",
    "    b'XOi/CzZ2TdEo279o8RYchjq/P0OZtGmgxtO/JezwgRUfyL83blLkT2yhP/knW//60uo/1fEmQXrO5b/2+0nprcHRv3vLkB5zzt6/'\n",
    "    b'19DiGucd3T/M79EZ2qXmv63cP7s/kdU/MSsrLva51b+MhISLNzjdv7Kw9U8d0eC/geJJfPp72j/zlqT5CIXMv7fb8xOe1IO/bkbV'\n",
    "    b'wBEO0T8lJPnG377gv+o7CHnhleC/1Qhtj/ZJpT+IJwjf7srqPwG3ZFVGus+/H3VAagNd1L/AxlnIbfr2vzYGDaCWLtK/B5QBlpJZ'\n",
    "    b'9b/BhKXLoaXjv/x0heWipOi/Sem05AlN5b/fK/5MeT7ev/XH6nI1a9K/28QbyMIu4r+cuOK2SWXyvwSSD7LmPcm/4cdGYiw62r80'\n",
    "    b'ZFYQs/0DwKmgrBRly9a/4J3ru6g3BcAcQAUQu8b2v2C1VwG7Pue/7swLWfwq0z+rJCsbpIPNv0MB99PE9sY/P2TEcWzp4b/oRKlM'\n",
    "    b'/JXKv5+O0sNMe+I/qz3oRYwh6L94gghIDufvv8xcrZb619i/nABHslxf8L/bB/6O4I30v+t/YsOD0uC/bF58i6Iv3L8B+DE7Wsb3'\n",
    "    b'P1/XDYzjKOO/CvVrQwC/4b+zkB8mNBndP6N8te73c4w/LaByY+YuvD++sT3s1H7Qvzz04N5Lesu/YnFoVUVdkr85E4jMjraXvxGb'\n",
    "    b'kuucnJq/dUkB69gLtj9fm91p0qDVvzQrCT8+oNM/V7k2cw/vuz/PbjpBIAi3v8mUtCcw/dK/O6P/YvXzxD9yZgOdPh3iv34KQUqg'\n",
    "    b'adM/atEm4hTJxD9EZC3renrCv4JwkSmHAtO/X2JvdLjZ4j+RROs4lg38v30HXm6IBck/a3HT1JvmxT9tLRZWofnRv6x/rrVosvO/',\n",
    "    shape=(COUNT_TESTS, INPUT_DIM + 1, OUTPUT_DIM)\n",
    ")\n",
    "RELU_OUTPUTS = decode_answer(\n",
    "    b'YiyQmO8F3j8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACg2CiUz2TzPwAAAAAAAAAA5CtrTHaEvj8AAAAAAAAAAAAAAAAAAAAAAAAA'\n",
    "    b'AAAAAAAAAAAAAAAAADJCr78bJvE/WS8D2voW5z8AAAAAAAAAAAAAAAAAAAAAaQxgASdm0T8AAAAAAAAAADv4UNwHJeI/V/k5EMmt'\n",
    "    b'0T8AAAAAAAAAAAAAAAAAAAAAIy7b6g8YvT8AAAAAAAAAAKTzoe+yzOA/ISOwt7Dm2T81IbokKXfiPwAAAAAAAAAAAAAAAAAAAAAA'\n",
    "    b'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAACSemmQTAnrPy1VcWBaNvE/AAAAAAAAAACF50HKCEz6PwAAAAAAAAAAwd68x2zZ1j8AAAAA'\n",
    "    b'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD6952Oveto/vCtLVNO00T8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA'\n",
    "    b'AAAAAAAAAAAAXxHvVLep7D85r0oej8TpPwAAAAAAAAAAt63DYXGGBEDB+lOtbOb2P/bXRrHncfU/AAAAAAAAAAAAAAAAAAAAAJBN'\n",
    "    b'ZdUcS9o/pBgdwQ8L6j8av8TZeuXAPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHkBh1wAFfI/AAAAAAAAAAAAAAAAAAAAAAaRztEK'\n",
    "    b'uvk/2udSvApj8D/gzG3RNDriPwujSd1sB+w/AAAAAAAAAACoQyp30y7vPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABgTDylweOE/'\n",
    "    b'AAAAAAAAAAAAAAAAAAAAAAzBuD87oug/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACEltIyyUPmP2Oqm8b53tU/iBCS'\n",
    "    b'chK27j8AAAAAAAAAAAAAAAAAAAAAtdJt8fMqwz8AAAAAAAAAAIi+ZPDzAeY/DW95bLqVxj/pR+S/0s/ZPwAAAAAAAAAAPraYpdBN'\n",
    "    b'0z8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAVSfL4M2f3PwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHGHHIgYB/U/9MSA1TkZ5j+m'\n",
    "    b'K3z4Rd3vP33JHTebLANAKp/kCql0jj8w2YC2AtwKQAAAAAAAAAAAAAAAAAAAAACmls8Rbq3sPwAAAAAAAAAAAAAAAAAAAAAAAAAA'\n",
    "    b'AAAAACSn9qtmWNg/oXMhsVm4tT9KDGAkRqzbPwY2K17MUfg/AAAAAAAAAABnNImjpzTjP5ENrHP7jNE/',\n",
    "    shape=(COUNT_TESTS, BATCH_SIZE, INPUT_DIM)\n",
    ")\n",
    "RELU_D_INPUTS = decode_answer(\n",
    "    b'A2itoRO18L8AAAAAAAAAgAAAAAAAAACAAAAAAAAAAIAHh/vZZPO/PwAAAAAAAACAWRIg0o5y1b8AAAAAAAAAgAAAAAAAAACAAAAA'\n",
    "    b'AAAAAAAAAAAAAAAAABE1RhH4o8o/KgQl1O5e5L8AAAAAAAAAAAAAAAAAAACAuuBfgdfy3T8AAAAAAAAAAOFo1PooyfQ/kqsXtUZd'\n",
    "    b'7z8AAAAAAAAAgAAAAAAAAACA0Ev2eU0CzD8AAAAAAAAAgDOyhn98d+k/RvVD7zz+sj8PYlvxi/bPvwAAAAAAAAAAAAAAAAAAAIAA'\n",
    "    b'AAAAAAAAAAAAAAAAAACAAAAAAAAAAAAS5p+ceXSxP4Kfa3xtPuk/AAAAAAAAAICRu8tJJc/tPwAAAAAAAACAVNcnv2uq8r8AAAAA'\n",
    "    b'AAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgFqConRSR+M/qWsKZIi7+b8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAAAAAAAAAA'\n",
    "    b'AAAAAAAAAAAARSt0hcOH1L+esNsN7Yn+PwAAAAAAAACAez+0wK/A3j8Itz0J1tjvv/ivP2za98a/AAAAAAAAAIAAAAAAAAAAAMVB'\n",
    "    b'rpc6rOk/hzNsPsw04D/jrFsZmwX6PwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgFLdAtr9fvi/AAAAAAAAAIAAAAAAAAAAAJkqln2f'\n",
    "    b'2t0/IChXD6IWwL+9Ou0Wx5z1Pz3sQ2fQrvO/AAAAAAAAAIDiIBDEYqMCQAAAAAAAAACAAAAAAAAAAIAAAAAAAAAAANEfbPk1BKY/'\n",
    "    b'AAAAAAAAAAAAAAAAAAAAAEvJuYm5Gt4/AAAAAAAAAIAAAAAAAAAAAAAAAAAAAACAAAAAAAAAAAB9zMZQP/PJP8BXEKpVDag/O7Yp'\n",
    "    b'MAiSwD8AAAAAAAAAAAAAAAAAAACApRbFTUOw6b8AAAAAAAAAAAG3xBTkt+y/vEFvt/tC/T+ejuHmq+/jvwAAAAAAAAAAKMW0BHXh'\n",
    "    b'1T8AAAAAAAAAgAAAAAAAAACAAAAAAAAAAICXmOxlXy7nPwAAAAAAAACAAAAAAAAAAAAAAAAAAAAAgPAWWwNPlIS/i9lVufSUz78O'\n",
    "    b'xmPOGt72v8/IkgrUPvS/3AyaWWXA5j9JW3lZsi7qvwAAAAAAAAAAAAAAAAAAAADWcSkNqkPUvwAAAAAAAACAAAAAAAAAAIAAAAAA'\n",
    "    b'AAAAAC6UQFNDQOy/SSAyS6fH/b8/A8h1B8Lnv1G0bA9o/v6/AAAAAAAAAIBUspOra+zrP+v3rx0Rmvi/',\n",
    "    shape=(COUNT_TESTS,  BATCH_SIZE, INPUT_DIM)\n",
    ")\n",
    "SOFTMAX_OUTPUTS = decode_answer(\n",
    "    b'xXhAbjms4T+kt8Giv6nQPzVFkyFmjrM/DBZi4c9ovD+9/2O0t+7iP44M7/No88I/U3O850ljyT+nBBJLubmvPxT87vyvibo/okfs'\n",
    "    b'spSinj/Z0qx5mibDP4qJX7wO8OY/l5LFH5o63z9Thj+aifG9P8BtGLOBd7U/ZXDkDCPr0z/IwiZOFI3EP5s57yGFtds/TWOx06O3'\n",
    "    b'1D/UBjCNMzG1PygFI9W5Y8I/d2S+q7Q01D+XoIyKpHKwP83wDEfFfN4/TZGKROcn2T/Z7S4nI+PdPxWaCFi/Kag/ZbYVpfa+tz/j'\n",
    "    b'y3AyoZfIPwqapYg9YrY/+uXC6quTsj8GvfaEWrvkP7TEueeHU9Q/U4IzYvzcuD9ssTWdee7hPxF+31RchJk/+H6EohVj4j+p1hFy'\n",
    "    b'9jbKP2yoaJx9abE/RtmnNfSHwz+EAnm6CdCyP3NM4EVc9dk/Phc5Ih2i1j9dt5BSCGnFP/e/vb1UidQ/etZ55g/8zD+ImpUDiLLU'\n",
    "    b'P4x035Y2jMA/EQnOG8JrwT82R0KU99O/P+R3sWe3B5E/FqlGy1Ii5z/OMhEaAWPdP0XDudD/1No/itb+kyNvoT9zvNSKami2PwTx'\n",
    "    b'EuhvENI/Gp3wsa8H2z/usO6aUlbLP6RlFGLc8rQ/puBG0pL5yT+xPT3zjBmxP2jJA2Dbs+U/oWst0OSoqj+hqzDFonKZP4UIrPFZ'\n",
    "    b'UuA/WIhcsW810j/Gt/B9ZB3HP5yWGOW8Bd0/WroVVtIvlT/YhpTupwPgP7UA0IVh/5k/orWs6WSIxz+sf3qzlWPQP3KcH9Yqm94/'\n",
    "    b'TiQ+BjT0tD/0gvjjYly3PyWmjDoWgeU/RA5ViJ4ftj/SnqZfpj3DP16af68gyMU/6Wf0OUhMxj/U69O12KfYPwkTctXyTdE/IWc2'\n",
    "    b'XM0B4j9ShSGuPi2yP3R+u6ZNrr0/z/BbMoIF0D+4D+jmodq3Pwm+Bal1rNg/IvxNqzOYzT/5P5kHyJDSP8mZuU3jR9U/d73rJi3M'\n",
    "    b'4D8+PRtUOHmmP4IOrmdtQrk/wpkNSXWMsj/UJG9Abf7pP8SFlAqk2qA/vfwurs4StT8zl5ubCdmzPyHFqQjqFdo/duHz1n72yz9X'\n",
    "    b'ZPUklPjSP8wdLsKQ99A/YYHty7gTmT/vJlfHtCvmP/PDSyai/JE/67D3fQJksT8qxg5hUlbiP3Ws7SgtYsI/amLbE4iSyz9NPV2T'\n",
    "    b'XXKbP4+nZp+yQ9Y/s4VBEkSR0D/p/iF143PXP4B2/H6DleE/eUUAMLjHsj+sRXJKPgnMP5q9m6HXPMQ/',\n",
    "    shape=(COUNT_TESTS, BATCH_SIZE, INPUT_DIM)\n",
    ")\n",
    "SOFTMAX_D_INPUTS = decode_answer(\n",
    "    b'WIS5wPxs4T/tYtAttQTgv9cCAzg36bG/Et7nguqbmj+RCJAVjLXIPzxW5KylgX2/ipaHs5Vswr9MPYXSpHOlvw2PEG2zW7y/JwA4'\n",
    "    b'6NU4ab+dTU/lBrmwP+ACBm7m2Kg/B/NST+5g3r9C2RHZsTzIP/yg5DkmurI/T7yhqBcoyz9vVDiNFadzPwpePGQmasi/iCAwIL87'\n",
    "    b'wT+C64leukSqPyixnj8Xe8I/k4hfLUpN0j9E/V2gBuGyPz0wo7qLIeC/r/IQwL1l3D/X0+Wf2/zhvxO113eJf7E/0j0mDrmgqT8m'\n",
    "    b'x0KV81DSP+DxONaMWbA/16nv9zr3tT8U7syIJeXbv6+7H1CZlbW/+coU9Og+vz+ZgoyXbui0v5nmLuc9fqY/dXkJ2kNvxL9McPuv'\n",
    "    b'eP7UP9BQ3+dJ66s/WTvl/3+IzL/IA9At1d6yv0xMvOWEcMo/2sbh6jB90b9LQ++GR/nBP/9ZUvw3m9s/GAO8VhNR3r/POg6w6VjA'\n",
    "    b'Pz3RdfZl2rW/qtEOh/cbuj+msDlAwpKyv2ydqxZGD6K/ANsKSNzmdz+QFp++o3vTPzcOtL/SJ9a/OQROdZy5ir9EPvsl3w+sP/ib'\n",
    "    b'Zs5/AsI/B6RNRqRM1D8Q8otoA47cv17+n54Y/Ie/rhdKMI9awT9+ILwcdYOuv8SymrvdwLi/aE6Spec1lT/4QPSzqMCevxRLlmqf'\n",
    "    b'Hsw/NqirnEao0b/0Gn/KBRS2Pz0YL5M80dO/jlzx7lGWgr/hgPyMrDDTP9wjolopVJM/NsC9m/sZur+zRdRP4eW3PypKj8jrO4+/'\n",
    "    b'Go/tE19umD9VAUaMLreoP73ULAeK0bw/jxu5zPBKsz99eAQNCTzOv2omI7+oHtW/OOU1Jclfx79YIGD5GNykP/oUkjIKM94/dtv2'\n",
    "    b'Bq3v2z+NWtKJvHm0v355k/lF6sW/MBBxzzW4x78PMyqAB/iYP/TdmjfU6am/gKn04JArmT+A9W3hAAFbPzf5e97n9bG/aDwuGZW5'\n",
    "    b'vL/eZWZDPuORv4vnQUQmlMk/sNL224W1mD9wTPCp6MeJPwyOtgLSt7c/IiZ5RxiPwL+Rrgnnpyinv3MTNHvFG8M/iGt/imN+nD8/'\n",
    "    b'lcHyZ+HAv31Hy8mFneG/gyw+x8uBkL80oAPJPw7jP1jqzB90lZ2/04p+0+MRnD/qQP8J1fvFv20rlcJDLJy//hTiByH/xT/qnEBS'\n",
    "    b'KCJ/P/B+GpViZ5C/eIHg7ViN07/0Jul1RhfUP8KBvfGu5bS/tIGJ1Kx4eD/V5GOKk927P2T23cve/qC/',\n",
    "    shape=(COUNT_TESTS,  BATCH_SIZE, INPUT_DIM)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_MSG = \"Error in test {}:\\ntrue values:{},\\ndesired_values:{}\"\n",
    "\n",
    "def check_answers(actual_values, desired_values, msg=''):\n",
    "    np.testing.assert_almost_equal(actual_values, desired_values, err_msg=msg, verbose=False)\n",
    "\n",
    "def test_forward(layer, true_outputs, set_weights=False, **kwargs):\n",
    "    layer.build()\n",
    "    for i, (input, true_output) in enumerate(zip(INPUTS, true_outputs)):\n",
    "        if set_weights:\n",
    "            layer.weights = WEIGHTS[i]\n",
    "        desired_output = layer.forward(input)\n",
    "        msg = ERROR_MSG.format(i, true_output, desired_output)\n",
    "        check_answers(true_output, desired_output, msg)\n",
    "        \n",
    "def test_backward(layer, true_d_inputs, d_outputs, true_mean_d_weights=None, **kwargs):\n",
    "    layer.build()\n",
    "    for i, (input, d_output, true_d_input) in enumerate(zip(INPUTS, d_outputs, true_d_inputs)):\n",
    "        if true_mean_d_weights is not None:\n",
    "            layer.weights = WEIGHTS[i]\n",
    "        layer.forward(input)\n",
    "        desired_d_input = layer.backward(d_output)\n",
    "        check_answers(true_d_input, desired_d_input, \n",
    "                      msg=ERROR_MSG.format(i, true_d_input, desired_d_input))\n",
    "        if true_mean_d_weights is not None:\n",
    "            check_answers(true_mean_d_weights[i], layer.mean_d_weights, \n",
    "                          msg=ERROR_MSG.format(i, true_mean_d_weights[i], layer.mean_d_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forward(Linear(OUTPUT_DIM, INPUT_DIM), LINEAR_OUTPUTS, set_weights=True)\n",
    "test_backward(Linear(OUTPUT_DIM, INPUT_DIM), LINEAR_D_INPUTS, LINEAR_D_OUTPUTS, LINEAR_MEAD_D_WEIGHTS)\n",
    "\n",
    "test_forward(ReLU(INPUT_DIM), RELU_OUTPUTS, set_weights=True)\n",
    "test_backward(ReLU(INPUT_DIM), RELU_D_INPUTS, RELU_D_OUTPUTS)\n",
    "\n",
    "test_forward(Softmax(INPUT_DIM), SOFTMAX_OUTPUTS, set_weights=True)\n",
    "test_backward(Softmax(INPUT_DIM), SOFTMAX_D_INPUTS, SOFTMAX_D_OUTPUTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Обучение нейросети стохастическим градиентным спуском"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теоретическая часть \n",
    "#### Градиентный спуск\n",
    "Настройка весов нейросети может быть сведена к поиску вектора $w$, доставляющего минимум функционалу ошибки:\n",
    "\n",
    "$$E(w) = E( \\boldsymbol{y},  \\boldsymbol{\\widehat{y}}) \\to min $$\n",
    "\n",
    "Минимизировать E(w) будем с помощью градиентного спуска. Правило изменения вектора весов на каждой итерации: \n",
    "\n",
    "$$\n",
    "w^{(k)} = w^{(k - 1)} - \\beta \\nabla_w E(w^{(k - 1)})\n",
    "$$\n",
    "Длину шага $\\beta > 0$ в рамках данного задания предлагается брать равной некоторой малой константе.\n",
    "\n",
    "В случае полного градиентного спуска $\\nabla_w E(w)$ считается все объекты выборки). В случае градиентного спуска  с минибатчем \n",
    "$$\\nabla_w E(w) \\approx \\frac{1}{n}\\sum_{j=1}^{n}{\\nabla_w q_{i_{k_j}} (w)}$$\n",
    "где $q_{i_{k_j}}$ — случайно выбранные номера слагаемых, а n меньше общего колличества примеров для обучения.\n",
    "\n",
    "#### Момент импульса(momentum)\n",
    "Может оказаться, что направление антиградиента сильно меняется от шага к шагу. Чтобы добиться болле эффективной сходимости, можно усреднять векторы антиградиента с нескольких предыдущих шагов — в этом случае шум уменьшится, и такой средний вектор будет указывать в сторону общего направления движения. Введем вектор инерции:\n",
    "    $$h_0 = 0;$$\n",
    "    $$h_k = \\alpha h_{k - 1} + \\beta \\nabla_w E(w^{(k - 1)}) $$\n",
    "Тогда шаг градиентного спуска будет:\n",
    "    $$w^{(k)} = w^{(k - 1)}  - h_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практическая часть\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3.** Реализация стохастического градиентного спуска с моментом\n",
    "\n",
    "\n",
    "Вам необходимо реализовать алгоритм обновления весов с моментом. Для этого необходимо реализовать метод `update_weights` класса `SGD`. Также вам необходимо реализовать метод `update_weights` во всех слоях нейросети(у которых есть веса)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    def __init__(self, lr, momentum=0):\n",
    "        self._lr = lr\n",
    "        self._momentum = momentum\n",
    "        self._h = 0\n",
    "\n",
    "    def update_weights(self, weights, gradient):\n",
    "        \"\"\"\n",
    "        weights: np.array((n, m)), current weigths of algorithm\n",
    "        gradient: np.array((n, m)), average gradient by weights\n",
    "        ---\n",
    "        output: np.array((n, m)), new weights values\n",
    "        \"\"\" \n",
    "        self._h = self._momentum * self._h + self._lr * gradient\n",
    "        return weights - self._h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4.** Обучение нейросети для задачи классификации цифр mnist\n",
    "\n",
    "В этой части вам необходимо обучить вашу нейросеть для задачи классификации рукописных цифр mnist. Вам потребуются методы `fit` и `evaluate` класса `Model`. Можете использовать предложенную архитектуру или выбрать любую другую. Шаг обучения, количество эпох и размер батча выбирите на ваше усмотрение. Посчитайте ошибку и точность(accuracy) предсказания на тестовом датасете.\n",
    "\n",
    "Обучите нейросеть без момента и c моментом(выбранным на ваше усмотрение) и постройте графики ошибки во время обучения в зависимости от числа итераций. Постройте те же графики для валидационной части. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-25-117ab9c84481>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Ekaterina\\Anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Ekaterina\\Anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Ekaterina\\Anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Ekaterina\\Anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Ekaterina\\Anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(images, labels):\n",
    "    binarizer = LabelBinarizer()\n",
    "    X = images.reshape((images.shape[0], 28 * 28))\n",
    "    y = binarizer.fit_transform(labels)\n",
    "    return X / 255.0, y \n",
    "\n",
    "def create_mnist_model(loss, optimizer):\n",
    "    model = Model(loss=loss, optimizer=optimizer)\n",
    "    model.add(Linear(10, input_dim=28*28))\n",
    "    model.add(ReLU())\n",
    "    model.add(Linear(10))\n",
    "    model.add(Softmax())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist.train.images, mnist.train.labels\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33)\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 0.183365, val loss: 0.184289\n",
      "Epoch: 2, train loss: 0.144297, val loss: 0.145389\n",
      "Epoch: 3, train loss: 0.111666, val loss: 0.112826\n",
      "Epoch: 4, train loss: 0.090803, val loss: 0.092078\n",
      "Epoch: 5, train loss: 0.077875, val loss: 0.079261\n",
      "Epoch: 6, train loss: 0.069318, val loss: 0.070781\n",
      "Epoch: 7, train loss: 0.063240, val loss: 0.064777\n",
      "Epoch: 8, train loss: 0.058688, val loss: 0.060267\n",
      "Epoch: 9, train loss: 0.055124, val loss: 0.056761\n",
      "Epoch: 10, train loss: 0.052289, val loss: 0.053972\n",
      "Epoch: 11, train loss: 0.049965, val loss: 0.051692\n",
      "Epoch: 12, train loss: 0.048013, val loss: 0.049768\n",
      "Epoch: 13, train loss: 0.046427, val loss: 0.048235\n",
      "Epoch: 14, train loss: 0.044972, val loss: 0.046815\n",
      "Epoch: 15, train loss: 0.043748, val loss: 0.045634\n",
      "Epoch: 16, train loss: 0.042720, val loss: 0.044655\n",
      "Epoch: 17, train loss: 0.041765, val loss: 0.043717\n",
      "Epoch: 18, train loss: 0.040903, val loss: 0.042890\n",
      "Epoch: 19, train loss: 0.040154, val loss: 0.042161\n",
      "Epoch: 20, train loss: 0.039478, val loss: 0.041505\n",
      "Epoch: 21, train loss: 0.038853, val loss: 0.040926\n",
      "Epoch: 22, train loss: 0.038298, val loss: 0.040380\n",
      "Epoch: 23, train loss: 0.037772, val loss: 0.039865\n",
      "Epoch: 24, train loss: 0.037334, val loss: 0.039476\n",
      "Epoch: 25, train loss: 0.036874, val loss: 0.039020\n",
      "Epoch: 26, train loss: 0.036471, val loss: 0.038622\n",
      "Epoch: 27, train loss: 0.036101, val loss: 0.038285\n",
      "Epoch: 28, train loss: 0.035742, val loss: 0.037948\n",
      "Epoch: 29, train loss: 0.035416, val loss: 0.037626\n",
      "Epoch: 30, train loss: 0.035106, val loss: 0.037348\n",
      "Epoch: 31, train loss: 0.034816, val loss: 0.037071\n",
      "Epoch: 32, train loss: 0.034547, val loss: 0.036823\n",
      "Epoch: 33, train loss: 0.034295, val loss: 0.036603\n",
      "Epoch: 34, train loss: 0.034060, val loss: 0.036351\n",
      "Epoch: 35, train loss: 0.033808, val loss: 0.036133\n",
      "Epoch: 36, train loss: 0.033597, val loss: 0.035937\n",
      "Epoch: 37, train loss: 0.033376, val loss: 0.035737\n",
      "Epoch: 38, train loss: 0.033177, val loss: 0.035542\n",
      "Epoch: 39, train loss: 0.032991, val loss: 0.035378\n",
      "Epoch: 40, train loss: 0.032806, val loss: 0.035210\n",
      "Epoch: 41, train loss: 0.032638, val loss: 0.035047\n",
      "Epoch: 42, train loss: 0.032458, val loss: 0.034872\n",
      "Epoch: 43, train loss: 0.032300, val loss: 0.034745\n",
      "Epoch: 44, train loss: 0.032148, val loss: 0.034596\n",
      "Epoch: 45, train loss: 0.032002, val loss: 0.034506\n",
      "Epoch: 46, train loss: 0.031847, val loss: 0.034323\n",
      "Epoch: 47, train loss: 0.031704, val loss: 0.034200\n",
      "Epoch: 48, train loss: 0.031567, val loss: 0.034062\n",
      "Epoch: 49, train loss: 0.031446, val loss: 0.033980\n",
      "Epoch: 50, train loss: 0.031307, val loss: 0.033843\n",
      "Epoch: 1, train loss: 0.055044, val loss: 0.056444\n",
      "Epoch: 2, train loss: 0.040144, val loss: 0.041884\n",
      "Epoch: 3, train loss: 0.035852, val loss: 0.037873\n",
      "Epoch: 4, train loss: 0.033624, val loss: 0.035850\n",
      "Epoch: 5, train loss: 0.032050, val loss: 0.034539\n",
      "Epoch: 6, train loss: 0.030847, val loss: 0.033520\n",
      "Epoch: 7, train loss: 0.029926, val loss: 0.032759\n",
      "Epoch: 8, train loss: 0.029502, val loss: 0.032473\n",
      "Epoch: 9, train loss: 0.029010, val loss: 0.032106\n",
      "Epoch: 10, train loss: 0.028546, val loss: 0.031731\n",
      "Epoch: 11, train loss: 0.027733, val loss: 0.031022\n",
      "Epoch: 12, train loss: 0.027802, val loss: 0.031248\n",
      "Epoch: 13, train loss: 0.027147, val loss: 0.030571\n",
      "Epoch: 14, train loss: 0.026492, val loss: 0.030080\n",
      "Epoch: 15, train loss: 0.026259, val loss: 0.029961\n",
      "Epoch: 16, train loss: 0.026101, val loss: 0.029950\n",
      "Epoch: 17, train loss: 0.025841, val loss: 0.029678\n",
      "Epoch: 18, train loss: 0.025566, val loss: 0.029548\n",
      "Epoch: 19, train loss: 0.025356, val loss: 0.029325\n",
      "Epoch: 20, train loss: 0.025152, val loss: 0.029215\n",
      "Epoch: 21, train loss: 0.024698, val loss: 0.028822\n",
      "Epoch: 22, train loss: 0.024563, val loss: 0.028859\n",
      "Epoch: 23, train loss: 0.024396, val loss: 0.028668\n",
      "Epoch: 24, train loss: 0.024106, val loss: 0.028503\n",
      "Epoch: 25, train loss: 0.024141, val loss: 0.028510\n",
      "Epoch: 26, train loss: 0.023814, val loss: 0.028294\n",
      "Epoch: 27, train loss: 0.023578, val loss: 0.028059\n",
      "Epoch: 28, train loss: 0.023331, val loss: 0.027857\n",
      "Epoch: 29, train loss: 0.023612, val loss: 0.028258\n",
      "Epoch: 30, train loss: 0.023052, val loss: 0.027759\n",
      "Epoch: 31, train loss: 0.023014, val loss: 0.027796\n",
      "Epoch: 32, train loss: 0.022718, val loss: 0.027556\n",
      "Epoch: 33, train loss: 0.022758, val loss: 0.027708\n",
      "Epoch: 34, train loss: 0.022746, val loss: 0.027700\n",
      "Epoch: 35, train loss: 0.022439, val loss: 0.027411\n",
      "Epoch: 36, train loss: 0.022404, val loss: 0.027385\n",
      "Epoch: 37, train loss: 0.022132, val loss: 0.027261\n",
      "Epoch: 38, train loss: 0.022301, val loss: 0.027515\n",
      "Epoch: 39, train loss: 0.022303, val loss: 0.027636\n",
      "Epoch: 40, train loss: 0.021779, val loss: 0.027029\n",
      "Epoch: 41, train loss: 0.021870, val loss: 0.027166\n",
      "Epoch: 42, train loss: 0.021703, val loss: 0.027117\n",
      "Epoch: 43, train loss: 0.021517, val loss: 0.027023\n",
      "Epoch: 44, train loss: 0.021582, val loss: 0.027227\n",
      "Epoch: 45, train loss: 0.021782, val loss: 0.027281\n",
      "Epoch: 46, train loss: 0.021412, val loss: 0.027161\n",
      "Epoch: 47, train loss: 0.021304, val loss: 0.027052\n",
      "Epoch: 48, train loss: 0.021074, val loss: 0.026847\n",
      "Epoch: 49, train loss: 0.021008, val loss: 0.026782\n",
      "Epoch: 50, train loss: 0.021189, val loss: 0.027054\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = create_mnist_model(loss=CategoricalCrossentropy(), optimizer=SGD(lr=0.01))\n",
    "model.fit(X_train, y_train, 30, 50, X_val=X_val, Y_val=y_val)\n",
    "loss_train_history_1 = np.array(model.loss_train_history)\n",
    "loss_val_history_1 = np.array(model.loss_val_history)\n",
    "loss_1 = model.evaluate(X_test, y_test, 1)\n",
    "\n",
    "model = create_mnist_model(loss=CategoricalCrossentropy(), optimizer=SGD(lr=0.01, momentum=0.9))\n",
    "model.fit(X_train, y_train, 30, 50, X_val=X_val, Y_val=y_val)\n",
    "loss_train_history_2 = np.array(model.loss_train_history)\n",
    "loss_val_history_2 = np.array(model.loss_val_history)\n",
    "loss_2 = model.evaluate(X_test, y_test, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При обучении обеих моделей val_loss на каждом шаге всегда немного больше, чем train_loss. При использовании momentum в SGD метод сходится быстрее и на тестовой выборке даёт меньшую ошибку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model1: lr=0.01, momentum=0; test_loss =  0.030822340723583943\n",
      "Model2: lr=0.01, momentum=0.9; test_loss =  0.02454480386532115\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Model1: lr=0.01, momentum=0; test_loss = ', loss_1)\n",
    "print('Model2: lr=0.01, momentum=0.9; test_loss = ', loss_2)\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.arange(loss_train_history_1.size), loss_train_history_1)\n",
    "plt.plot(np.arange(loss_train_history_2.size), loss_train_history_2)\n",
    "plt.title('Train loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('n_epoches')\n",
    "plt.legend(['lr=0.01, momentum=0', 'lr=0.01, momentum=0.9'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.arange(loss_val_history_1.size), loss_val_history_1)\n",
    "plt.plot(np.arange(loss_val_history_2.size), loss_val_history_2)\n",
    "plt.title('Val loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('n_epoches')\n",
    "plt.legend(['lr=0.01, momentum=0', 'lr=0.01, momentum=0.9'])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
